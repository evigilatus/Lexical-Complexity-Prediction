{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lcp_multi_train.tsv', 'lcp_single_train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import spatial\n",
    "print(os.listdir('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_single_tsv = 'train/lcp_single_train.tsv'\n",
    "df_train_single = pd.read_csv(train_single_tsv, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'corpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 7232\n",
      "Subcorpus len:\n",
      "\n",
      "biomed      2576\n",
      "europarl    2512\n",
      "bible       2144\n",
      "Name: corpus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_train_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_train_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_train_single['corpus'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_w2v_loc = 'InferSent/glove.6B.300d.txt'\n",
    "with open(glove_w2v_loc,  \"r\", encoding=\"utf8\") as lines:\n",
    "    glove_w2v = {}\n",
    "    for line in lines:\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        vector = [float(i) for i in split[1:]]\n",
    "        glove_w2v[word.lower()] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(glove_w2v.keys(), key=lambda word: spatial.distance.euclidean(glove_w2v[word.lower()], embedding))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of closest words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['baby', 'babies', 'newborn', 'infant', 'birth']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of closest words:\")\n",
    "find_closest_embeddings(glove_w2v['baby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of word arithmetics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['king', 'queen', 'monarch', 'mother', 'princess']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of word arithmetics:\")\n",
    "find_closest_embeddings(np.array(glove_w2v['king']) - np.array(glove_w2v['man']) + np.array(glove_w2v['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king', 'queen', 'monarch', 'prince', 'kingdom']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(np.array(glove_w2v['king']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['monk', 'nun', 'woman', 'nuns', 'monks']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(np.array(glove_w2v['monk']) - np.array(glove_w2v['man']) + np.array(glove_w2v['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of word arithmetics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['europe', 'country', 'countries', 'european', 'nation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of word arithmetics:\")\n",
    "find_closest_embeddings(np.array(glove_w2v['country']) + np.array(glove_w2v['europe']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of InferSent\n",
    "https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8\n",
    "\n",
    "\n",
    "https://research.fb.com/downloads/infersent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InferSent.models import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pkl = 'InferSent/encoder/infersent1.pkl'\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 1}\n",
    "infer_sent_model = InferSent(params_model)\n",
    "infer_sent_model.load_state_dict(torch.load(model_pkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "infer_sent_model.set_w2v_path(glove_w2v_loc)\n",
    "infer_sent_model.build_vocab_k_words(K=100000)\n",
    "\n",
    "# infer_sent_model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17863286,  0.08774211,  0.05200031, ...,  0.00108394,\n",
       "        -0.0540266 ,  0.03372176]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_sent_model.encode([\"This man is playing computer games\"], tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for single prediction: 0.0425105094909668\n",
      "Time for pred of 3000 cases: 5.273191928863525\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_for_context(ctx):\n",
    "    if not isinstance(ctx, list):\n",
    "#       print(\"ctx is not list\")\n",
    "        ctx = [ctx]\n",
    "    return infer_sent_model.encode(ctx, tokenize=True)\n",
    "\n",
    "start = time.time()\n",
    "get_embedding_for_context(\"This is a test sentence\")\n",
    "print(\"Time for single prediction: {}\".format(time.time() - start))\n",
    "\n",
    "get_embedding_for_context([\"This is a test sentence\"] * 3000)\n",
    "print(\"Time for pred of 3000 cases: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding time for all sentences: 88.85007405281067\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all sentence embeddings for the data:\n",
    "sentence_embeddings = {}\n",
    "idx = 600\n",
    "all_sentences = df_train_single['sentence'].tolist()\n",
    "all_sentences[idx] == df_train_single.loc[idx, 'sentence']\n",
    "\n",
    "start = time.time()\n",
    "all_sentence_embeddings = get_embedding_for_context(all_sentences)\n",
    "print(\"Encoding time for all sentences: {}\".format(time.time() - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7232\n",
      "7232\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sentences))\n",
    "print(len(all_sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7978083]]\n",
      "[[0.8062569]]\n",
      "[[0.63243806]]\n",
      "[[0.6913799]]\n",
      "[[0.8188079]]\n",
      "[[0.7767902]]\n",
      "[[0.8434121]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def measure_dist_between_ctx(c1, c2):\n",
    "    e1 = get_embedding_for_context(c1)[0]\n",
    "    e2 = get_embedding_for_context(c2)[0]\n",
    "    #return spatial.distance.euclidean(e1, e2)\n",
    "    return cosine_similarity([e1], [e2])\n",
    "\n",
    "print(measure_dist_between_ctx(\"In India people are going to war.\", \"The family went to an indian restaurant.\"))\n",
    "print(measure_dist_between_ctx(\"The baby is hungry.\", \"The child needs to eat.\"))\n",
    "print(measure_dist_between_ctx(\"Programming takes ages to master.\", \"Ronaldo scored a goal against man united.\"))\n",
    "print(measure_dist_between_ctx(\"At the university students go to lectures.\", \"Ronaldo scored a goal against man united.\"))\n",
    "print(measure_dist_between_ctx(\"A soccer game with multiple males playing.\", \"Some men are playing a sport.\"))\n",
    "print(measure_dist_between_ctx(\"The man is cooking chicken with potatoes.\", \"A man is driving down a lonely road.\"))\n",
    "print(measure_dist_between_ctx(\"The man is cooking chicken with potatoes.\", \"In the restaurant they serve delicious food.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "# According to the paper there are 3 handcrafted features\n",
    "# - word lenghth\n",
    "# - word frequency (TODO)\n",
    "# - syllable count\n",
    "def get_handcrafted_features(word):\n",
    "    word = str(word)\n",
    "    return [len(word), syllables.estimate(word)]\n",
    "\n",
    "get_handcrafted_features(\"Basketball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([ 4.0000,  1.0000,  0.0877,  ..., -0.2311, -0.5910,  0.4979]) Input Length:  4398\n",
      "Output:  tensor([0.1607])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CompLexDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(df_train_single)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "        \n",
    "        token = df_train_single.loc[idx, 'token']\n",
    "        token = str(token)\n",
    "        out = df_train_single.loc[idx, 'complexity']\n",
    "        \n",
    "        handcrafted_features = get_handcrafted_features(token)\n",
    "        sentence_ctx = all_sentence_embeddings[idx]\n",
    "        \n",
    "        if token.lower() in glove_w2v:   \n",
    "            w2v_for_token = glove_w2v[token.lower()]\n",
    "        else:\n",
    "            #print(\"Token {} not found\".format(token.lower()))\n",
    "            w2v_for_token = [0] * 300\n",
    "        \n",
    "        \n",
    "        result = {\n",
    "            'inp': torch.from_numpy(np.hstack((np.array(handcrafted_features), sentence_ctx, np.array(w2v_for_token))).ravel()).float(),\n",
    "            'out': torch.from_numpy(np.array([out])).float()\n",
    "        }\n",
    "        \n",
    "        #print(\"Idx {} fetch time: {}\".format(idx, time.time() - start))\n",
    "        return result\n",
    "    \n",
    "\n",
    "dataset = CompLexDataset()\n",
    "\n",
    "print(\"Input: \", dataset[5]['inp'], \"Input Length: \", len(dataset[5]['inp']))\n",
    "print(\"Output: \", dataset[5]['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inp': tensor([5.0000, 2.0000, 0.1231,  ..., 0.5989, 0.3270, 0.6747]),\n",
       " 'out': tensor([0.])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(dataset[0]['inp']), 1600)\n",
    "        self.b1 = nn.BatchNorm1d(1600)\n",
    "        self.fc2 = nn.Linear(1600, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        #x = self.b1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "net = Network()\n",
    "#net.to(torch.device(\"cuda:0\"))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, criterion):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.is_available())\n",
    "#print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 3.4574625492095947\n",
      "Epoch 2 Loss : 0.9443999528884888\n",
      "Epoch 3 Loss : 0.9375672340393066\n",
      "Epoch 4 Loss : 0.9005191922187805\n",
      "Epoch 5 Loss : 0.8740091323852539\n",
      "Epoch 6 Loss : 0.8657763004302979\n",
      "Epoch 7 Loss : 0.8575523495674133\n",
      "Epoch 8 Loss : 0.8625211119651794\n",
      "Epoch 9 Loss : 0.8669983744621277\n",
      "Epoch 10 Loss : 0.827931821346283\n",
      "Epoch 11 Loss : 0.8392904996871948\n",
      "Epoch 12 Loss : 0.8486968874931335\n",
      "Epoch 13 Loss : 0.8226974010467529\n",
      "Epoch 14 Loss : 0.8279427289962769\n",
      "Epoch 15 Loss : 0.803523600101471\n",
      "Epoch 16 Loss : 0.8364858627319336\n",
      "Epoch 17 Loss : 0.8414934873580933\n",
      "Epoch 18 Loss : 0.8348140716552734\n",
      "Epoch 19 Loss : 0.7871792912483215\n",
      "Epoch 20 Loss : 0.7947569489479065\n",
      "Epoch 21 Loss : 0.8353956937789917\n",
      "Epoch 22 Loss : 0.8091203570365906\n",
      "Epoch 23 Loss : 0.8333479762077332\n",
      "Epoch 24 Loss : 0.8229992389678955\n",
      "Epoch 25 Loss : 0.8168485164642334\n",
      "Epoch 26 Loss : 0.8169122338294983\n",
      "Epoch 27 Loss : 0.8129745125770569\n",
      "Epoch 28 Loss : 0.7855275869369507\n",
      "Epoch 29 Loss : 0.8218967318534851\n",
      "Epoch 30 Loss : 0.8101266622543335\n",
      "Epoch 31 Loss : 0.7959774732589722\n",
      "Epoch 32 Loss : 0.856330394744873\n",
      "Epoch 33 Loss : 0.8122655749320984\n",
      "Epoch 34 Loss : 0.8417920470237732\n",
      "Epoch 35 Loss : 0.8368643522262573\n",
      "Epoch 36 Loss : 0.8186210989952087\n",
      "Epoch 37 Loss : 0.822810173034668\n",
      "Epoch 38 Loss : 0.7697979211807251\n",
      "Epoch 39 Loss : 0.7981262803077698\n",
      "Epoch 40 Loss : 0.8350455164909363\n",
      "Epoch 41 Loss : 0.9069199562072754\n",
      "Epoch 42 Loss : 0.7788639068603516\n",
      "Epoch 43 Loss : 0.8683265447616577\n",
      "Epoch 44 Loss : 0.8359355330467224\n",
      "Epoch 45 Loss : 0.7986974716186523\n",
      "Epoch 46 Loss : 0.7978670597076416\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 46\n",
    "BATCH_SIZE = 64\n",
    "optm = Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "data_train = DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for bidx, batch in enumerate(data_train):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_train = batch['out']\n",
    "        #print(\"Fetch time: {}\".format(time.time() - start))\n",
    "        \n",
    "        #start = time.time()\n",
    "        loss, predictions = train(net,x_train,y_train, optm, criterion)\n",
    "        epoch_loss+=loss\n",
    "        #print(\"Predict time: {}\".format(time.time() - start))\n",
    "        \n",
    "    print('Epoch {} Loss : {}'.format((epoch+1),epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement MAE for the NN and for a random values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
