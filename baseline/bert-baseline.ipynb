{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\projects\\Lexical-Complexity-Prediction\\baseline\n",
      "['lcp_multi_train.tsv', 'lcp_single_train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import spatial\n",
    "print(os.getcwd())\n",
    "print(os.listdir('../dataset/train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_tsv = '../dataset/train/lcp_single_train.tsv'\n",
    "df_train_single = pd.read_csv(train_single_tsv, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'corpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 7662\n",
      "Subcorpus len:\n",
      "\n",
      "biomed      2576\n",
      "bible       2574\n",
      "europarl    2512\n",
      "Name: corpus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_train_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_train_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_train_single['corpus'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_tsv = '../dataset/test/lcp_single_test.tsv'\n",
    "df_test_single = pd.read_csv(test_single_tsv, sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'corpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 917\n",
      "Subcorpus len:\n",
      "\n",
      "europarl    345\n",
      "biomed      289\n",
      "bible       283\n",
      "Name: corpus, dtype: int64\n",
      "C:\\projects\\Lexical-Complexity-Prediction\\baseline\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_test_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_test_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_test_single['corpus'].value_counts())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_single_tsv = '../dataset/trial/lcp_single_trial.tsv'\n",
    "df_trial_single = pd.read_csv(trial_single_tsv, sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'subcorpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 421\n",
      "Subcorpus len:\n",
      "\n",
      "europarl    143\n",
      "bible       143\n",
      "biomed      135\n",
      "Name: subcorpus, dtype: int64\n",
      "C:\\projects\\Lexical-Complexity-Prediction\\baseline\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_trial_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_trial_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_trial_single['subcorpus'].value_counts())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('stsb-distilbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for single prediction: 0.41704225540161133\n",
      "Time for pred of 3000 cases: 2.1432719230651855\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_for_context(ctx):\n",
    "    if not isinstance(ctx, list):\n",
    "#       print(\"ctx is not list\")\n",
    "        ctx = [ctx]\n",
    "    return sbert_model.encode(ctx)\n",
    "\n",
    "start = time.time()\n",
    "get_embedding_for_context(\"This is a test sentence\")\n",
    "print(\"Time for single prediction: {}\".format(time.time() - start))\n",
    "\n",
    "get_embedding_for_context([\"This is a test sentence\"] * 3000)\n",
    "print(\"Time for pred of 3000 cases: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted features\n",
    "\n",
    "* Word length\n",
    "* Syllable count\n",
    "* Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3, 4.4409521980296365]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "# According to the paper there are 3 handcrafted features\n",
    "# - word length\n",
    "# - word frequency \n",
    "# - syllable count\n",
    "import csv\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "reader = csv.reader(open('SUBTLEX.csv', 'r'))\n",
    "frequency = defaultdict(float)\n",
    "frequency_count = dict()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for row in reader:\n",
    "    token = stemmer.stem(row[0].lower())\n",
    "    frequency[token] += float(row[5])\n",
    "    \n",
    "#     if token in frequency_count:\n",
    "#         frequency_count[token] += 1\n",
    "#     else:\n",
    "#         frequency_count[token] = 1\n",
    "    \n",
    "# for key in frequency:\n",
    "#     frequency[key] = frequency[key]/frequency_count[key]\n",
    "\n",
    "frequency = {k: math.log2(v) for k, v in frequency.items()}\n",
    "\n",
    "def get_handcrafted_features(word):\n",
    "    word = str(word)\n",
    "    return [len(word), syllables.estimate(word), frequency.get(stemmer.stem(word.lower())) or 0]\n",
    "\n",
    "get_handcrafted_features(\"Basketball\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_embeddings(dataset):\n",
    "    # Preprocess all sentence embeddings for the data:\n",
    "    sentence_embeddings = {}\n",
    "    \n",
    "    all_sentences = dataset['sentence'].tolist()\n",
    "\n",
    "    start = time.time()\n",
    "    all_sentence_embeddings = get_embedding_for_context(all_sentences)\n",
    "    print(\"Encoding time for all sentences: {}\".format(time.time() - start))\n",
    "    return all_sentence_embeddings\n",
    "    \n",
    "\n",
    "class CompLexDataset(Dataset):\n",
    "    global dataset_type\n",
    "    \n",
    "    def __init__(self, dataset_type):\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "        if(self.dataset_type == 'train'):                   \n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_train_single)\n",
    "        elif(self.dataset_type == 'trial'):\n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_trial_single)\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_test_single)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.dataset_type == 'train'):                   \n",
    "            return len(df_train_single)\n",
    "        elif(self.dataset_type == 'trial'):\n",
    "            return len(df_trial_single)\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            return len(df_test_single)\n",
    "        else: \n",
    "            raise Exception(\"Invalid dataset type.\", self.dataset_type)\n",
    "\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        start = time.time()\n",
    "        if(self.dataset_type == 'train'):\n",
    "            token = df_train_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_train_single.loc[idx, 'complexity']\n",
    "        elif(self.dataset_type == 'trial'):\n",
    "            token = df_trial_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_trial_single.loc[idx, 'complexity']\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            token = df_test_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_test_single.loc[idx, 'complexity']\n",
    "        else: \n",
    "            raise Exception(\"Invalid dataset type.\", self.dataset_type)\n",
    "        \n",
    "        handcrafted_features = get_handcrafted_features(token)\n",
    "\n",
    "        sentence_ctx = self.all_sentence_embeddings[idx]\n",
    "        \n",
    "#         if token.lower() in glove_w2v:   \n",
    "#             w2v_for_token = glove_w2v[token.lower()]\n",
    "#         else:\n",
    "#             #print(\"Token {} not found\".format(token.lower()))\n",
    "#             w2v_for_token = [0] * 300\n",
    "        \n",
    "        \n",
    "        result = {\n",
    "            'inp': torch.from_numpy(np.hstack((np.array(handcrafted_features), sentence_ctx)).ravel()).float(), \n",
    "            'out': torch.from_numpy(np.array([out])).float()\n",
    "        }\n",
    "        \n",
    "        #print(\"Idx {} fetch time: {}\".format(idx, time.time() - start))\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding time for all sentences: 25.441973209381104\n",
      "Encoding time for all sentences: 1.4798219203948975\n",
      "Encoding time for all sentences: 3.0330488681793213\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CompLexDataset(\"train\")\n",
    "trial_dataset = CompLexDataset(\"trial\")\n",
    "test_dataset = CompLexDataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(train_dataset[0]['inp']), 1600)\n",
    "        self.fc2 = nn.Linear(1600, 1600)\n",
    "        self.fc3 = nn.Linear(1600, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        #return x\n",
    "        return self.sigmoid(x)\n",
    "        \n",
    "    \n",
    "net = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, criterion):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    " #   print(\"output\", output)\n",
    " #   print(\"y\", y)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error ###\n",
    "Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 1.5881422758102417\n",
      "Epoch 2 Loss : 1.1757149696350098\n",
      "Epoch 3 Loss : 1.153933048248291\n",
      "Epoch 4 Loss : 1.1381193399429321\n",
      "Epoch 5 Loss : 1.1303942203521729\n",
      "Epoch 6 Loss : 1.1176087856292725\n",
      "Epoch 7 Loss : 1.1048948764801025\n",
      "Epoch 8 Loss : 1.1002025604248047\n",
      "Epoch 9 Loss : 1.0955835580825806\n",
      "Epoch 10 Loss : 1.0947943925857544\n",
      "Epoch 11 Loss : 1.0883023738861084\n",
      "Epoch 12 Loss : 1.0804123878479004\n",
      "Epoch 13 Loss : 1.080855131149292\n",
      "Epoch 14 Loss : 1.073858618736267\n",
      "Epoch 15 Loss : 1.069102168083191\n",
      "Epoch 16 Loss : 1.0711023807525635\n",
      "Epoch 17 Loss : 1.0651047229766846\n",
      "Epoch 18 Loss : 1.0682086944580078\n",
      "Epoch 19 Loss : 1.064536690711975\n",
      "Epoch 20 Loss : 1.058791995048523\n",
      "Epoch 21 Loss : 1.0599945783615112\n",
      "Epoch 22 Loss : 1.0582911968231201\n",
      "Epoch 23 Loss : 1.0593011379241943\n",
      "Epoch 24 Loss : 1.0588645935058594\n",
      "Epoch 25 Loss : 1.0510766506195068\n",
      "Epoch 26 Loss : 1.0501083135604858\n",
      "Epoch 27 Loss : 1.0516985654830933\n",
      "Epoch 28 Loss : 1.055222511291504\n",
      "Epoch 29 Loss : 1.052940845489502\n",
      "Epoch 30 Loss : 1.0495647192001343\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "optm = Adam(net.parameters(), lr = 0.00001)\n",
    "\n",
    "data_train = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for bidx, batch in enumerate(data_train):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_train = batch['out']\n",
    "        #print(\"Fetch time: {}\".format(time.time() - start))\n",
    "        \n",
    "        #start = time.time()\n",
    "        loss, predictions = train(net,x_train,y_train, optm, criterion)\n",
    "        epoch_loss += loss\n",
    "        #print(\"Predict time: {}\".format(time.time() - start))\n",
    "        \n",
    "    print('Epoch {} Loss : {}'.format((epoch+1),epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2263], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(train_dataset[210]['inp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# y_true = [test_dataset[i]['out'].item() for i in range(len(test_dataset))]\n",
    "# y_pred = []\n",
    "\n",
    "# test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "# for bidx, batch in enumerate(test_loader):\n",
    "#         #start = time.time()\n",
    "#         x_train = batch['inp']\n",
    "#         y_pred.append(net(x_train))\n",
    "\n",
    "# y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "# mae = mean_absolute_error(y_true, y_pred)\n",
    "# print(\"MAE for test data: \", mae)\n",
    "\n",
    "# with open('test_results.csv', 'w', newline='') as f:\n",
    "#     f_writer = csv.writer(f, delimiter=',',)\n",
    "#     for idx in range(len(df_test_single)):\n",
    "#        f_writer.writerow((df_test_single.loc[idx, 'id'], str(y_pred[idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for trial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for trial data:  0.07371862768029656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [trial_dataset[i]['out'].item() for i in range(len(trial_dataset))]\n",
    "y_pred = []\n",
    "\n",
    "trial_loader = DataLoader(dataset = trial_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "for bidx, batch in enumerate(trial_loader):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_pred.append(net(x_train))\n",
    "\n",
    "y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE for trial data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for train data:  0.07115381368572299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [train_dataset[i]['out'].item() for i in range(len(train_dataset))]\n",
    "y_pred = []\n",
    "\n",
    "test_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "for bidx, batch in enumerate(test_loader):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_pred.append(net(x_train))\n",
    "\n",
    "y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE for train data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for total random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for train data:  0.30674491430737033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "\n",
    "y_true = [train_dataset[i]['out'].item() for i in range(len(train_dataset))]\n",
    "y_pred = [random.random() for i in range(len(train_dataset))]\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error for train data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence, token):\n",
    "    sentence_embeddings = get_embedding_for_context(sentence)[0]\n",
    "    handcrafted_features = get_handcrafted_features(token)\n",
    "            \n",
    "\n",
    "    return {\n",
    "            'inp': torch.from_numpy(np.hstack((np.array(handcrafted_features), sentence_embeddings))).ravel().float() \n",
    "           }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:  Peter loves pineapples and apples! \n",
      "TOKEN:  pineapples \n",
      "COMPLEXITY:  0.34796950221061707 \n",
      "\n",
      "SENTENCE:  Peter loves pineapples and apples! \n",
      "TOKEN:  apples \n",
      "COMPLEXITY:  0.2773303985595703 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Peter loves pineapples and apples!'\n",
    "token1 = 'pineapples'\n",
    "token2 = 'apples'\n",
    "\n",
    "y_pred1 = net(prepare_sentence(sentence, token1)['inp'])\n",
    "print('SENTENCE: ', sentence, '\\nTOKEN: ', token1, '\\nCOMPLEXITY: ', y_pred1.item(), '\\n')\n",
    "\n",
    "y_pred2 = net(prepare_sentence(sentence, token2)['inp'])\n",
    "print('SENTENCE: ', sentence, '\\nTOKEN: ', token2, '\\nCOMPLEXITY: ', y_pred2.item(), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
