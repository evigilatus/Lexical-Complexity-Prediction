{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lcp_multi_train.tsv', 'lcp_single_train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import spatial\n",
    "print(os.listdir('train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_tsv = 'train/lcp_single_train.tsv'\n",
    "df_train_single = pd.read_csv(train_single_tsv, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'corpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 7232\n",
      "Subcorpus len:\n",
      "\n",
      "biomed      2576\n",
      "europarl    2512\n",
      "bible       2144\n",
      "Name: corpus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_train_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_train_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_train_single['corpus'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_tsv = 'test/lcp_single_trial.tsv'\n",
    "df_test_single = pd.read_csv(test_single_tsv, sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'subcorpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 311\n",
      "Subcorpus len:\n",
      "\n",
      "europarl    143\n",
      "bible       124\n",
      "biomed       44\n",
      "Name: subcorpus, dtype: int64\n",
      "/home/borisovai/Documents/AI/NLP_Course/SemEval2021\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_test_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_test_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_test_single['subcorpus'].value_counts())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe ###\n",
    "Load the pretrained GloVe vectors and verify that the operation has been successful by some quick experiments with the embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702835  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_w2v_loc = 'InferSent/GloVe/glove.840B.300d.txt'\n",
    "with open(glove_w2v_loc,  \"r\", encoding=\"utf8\") as lines:\n",
    "    glove_w2v = {}\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        vector = np.asarray(values[-300:], dtype='float32')\n",
    "        glove_w2v[word.lower()] = vector\n",
    "    print(len(glove_w2v),\" words loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(glove_w2v.keys(), key=lambda word: spatial.distance.euclidean(glove_w2v[word.lower()], embedding))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of closest words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['baby', 'rank_7', 'pushposters.com', 'seitp202', '765.361.6100']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of closest words:\")\n",
    "find_closest_embeddings(glove_w2v['baby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of word arithmetics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['woman', 'king', 'halfsugar', 'mattjgilbert', 'zephyp']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of word arithmetics:\")\n",
    "find_closest_embeddings(np.array(glove_w2v['king']) - np.array(glove_w2v['man']) + np.array(glove_w2v['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferSent\n",
    "- https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8\n",
    "- https://research.fb.com/downloads/infersent/\n",
    "\n",
    "Load InferSent model and execute some experiments.  \n",
    "**To Do:** Currently it is using GloVe. We should choose between GloVe or fastText vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InferSent.models import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pkl = 'InferSent/encoder/infersent1.pkl'\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 1}\n",
    "infer_sent_model = InferSent(params_model)\n",
    "infer_sent_model.load_state_dict(torch.load(model_pkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "infer_sent_model.set_w2v_path(glove_w2v_loc)\n",
    "infer_sent_model.build_vocab_k_words(K=100000)\n",
    "\n",
    "# infer_sent_model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08556896, -0.02621041,  0.10144135, ..., -0.03926745,\n",
       "        -0.03814263, -0.02820691]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_sent_model.encode([\"This man is playing computer games\"], tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for single prediction: 0.04985833168029785\n",
      "Time for pred of 3000 cases: 8.740787982940674\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_for_context(ctx):\n",
    "    if not isinstance(ctx, list):\n",
    "#       print(\"ctx is not list\")\n",
    "        ctx = [ctx]\n",
    "    return infer_sent_model.encode(ctx, tokenize=True)\n",
    "\n",
    "start = time.time()\n",
    "get_embedding_for_context(\"This is a test sentence\")\n",
    "print(\"Time for single prediction: {}\".format(time.time() - start))\n",
    "\n",
    "get_embedding_for_context([\"This is a test sentence\"] * 3000)\n",
    "print(\"Time for pred of 3000 cases: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7232\n",
      "7232\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sentences))\n",
    "print(len(all_sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47350663]]\n",
      "[[0.6629041]]\n",
      "[[0.39011022]]\n",
      "[[0.34307462]]\n",
      "[[0.73668134]]\n",
      "[[0.4258361]]\n",
      "[[0.6617247]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def measure_dist_between_ctx(c1, c2):\n",
    "    e1 = get_embedding_for_context(c1)[0]\n",
    "    e2 = get_embedding_for_context(c2)[0]\n",
    "    #return spatial.distance.euclidean(e1, e2)\n",
    "    return cosine_similarity([e1], [e2])\n",
    "\n",
    "print(measure_dist_between_ctx(\"In India people are going to war.\", \"The family went to an indian restaurant.\"))\n",
    "print(measure_dist_between_ctx(\"The baby is hungry.\", \"The child needs to eat.\"))\n",
    "print(measure_dist_between_ctx(\"Programming takes ages to master.\", \"Ronaldo scored a goal against man united.\"))\n",
    "print(measure_dist_between_ctx(\"At the university students go to lectures.\", \"Ronaldo scored a goal against man united.\"))\n",
    "print(measure_dist_between_ctx(\"A soccer game with multiple males playing.\", \"Some men are playing a sport.\"))\n",
    "print(measure_dist_between_ctx(\"The man is cooking chicken with potatoes.\", \"A man is driving down a lonely road.\"))\n",
    "print(measure_dist_between_ctx(\"The man is cooking chicken with potatoes.\", \"In the restaurant they serve delicious food.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted features\n",
    "\n",
    "* Word length\n",
    "* Syllable count\n",
    "* **To Do:** word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "# According to the paper there are 3 handcrafted features\n",
    "# - word lenghth\n",
    "# - word frequency (TODO)\n",
    "# - syllable count\n",
    "def get_handcrafted_features(word):\n",
    "    word = str(word)\n",
    "    return [len(word), syllables.estimate(word)]\n",
    "\n",
    "get_handcrafted_features(\"Basketball\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_embeddings(dataset):\n",
    "    # Preprocess all sentence embeddings for the data:\n",
    "    sentence_embeddings = {}\n",
    "    \n",
    "    all_sentences = dataset['sentence'].tolist()\n",
    "\n",
    "    start = time.time()\n",
    "    all_sentence_embeddings = get_embedding_for_context(all_sentences)\n",
    "    print(\"Encoding time for all sentences: {}\".format(time.time() - start))\n",
    "    return all_sentence_embeddings\n",
    "    \n",
    "\n",
    "class CompLexDataset(Dataset):\n",
    "    global dataset_type\n",
    "    \n",
    "    def __init__(self, dataset_type):\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "        if(self.dataset_type == 'train'):                   \n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_train_single)\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_test_single)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.dataset_type == 'train'):                   \n",
    "            return len(df_train_single)\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            return len(df_test_single)\n",
    "        else: \n",
    "            raise Exception(\"Invalid dataset type.\", self.dataset_type)\n",
    "\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        start = time.time()\n",
    "        if(self.dataset_type == 'train'):\n",
    "            token = df_train_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_train_single.loc[idx, 'complexity']\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            token = df_test_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_test_single.loc[idx, 'complexity']\n",
    "        else: \n",
    "            raise Exception(\"Invalid dataset type.\", self.dataset_type)\n",
    "        \n",
    "        handcrafted_features = get_handcrafted_features(token)\n",
    "        sentence_ctx = self.all_sentence_embeddings[idx]\n",
    "        \n",
    "        if token.lower() in glove_w2v:   \n",
    "            w2v_for_token = glove_w2v[token.lower()]\n",
    "        else:\n",
    "            #print(\"Token {} not found\".format(token.lower()))\n",
    "            w2v_for_token = [0] * 300\n",
    "        \n",
    "        \n",
    "        result = {\n",
    "            'inp': torch.from_numpy(np.hstack((np.array(handcrafted_features), sentence_ctx, np.array(w2v_for_token))).ravel()).float(), \n",
    "            'out': torch.from_numpy(np.array([out])).float()\n",
    "        }\n",
    "        \n",
    "        #print(\"Idx {} fetch time: {}\".format(idx, time.time() - start))\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding time for all sentences: 162.75237131118774\n",
      "Input:  tensor([ 4.0000,  1.0000,  0.0964,  ..., -0.5811, -0.1337, -0.7521]) Input Length:  4398\n",
      "Output:  tensor([0.1607])\n",
      "Encoding time for all sentences: 27.465412616729736\n",
      "Input:  tensor([ 4.0000,  1.0000,  0.0798,  ...,  0.2892, -0.4453, -0.6912]) Input Length:  4398\n",
      "Output:  tensor([0.0250])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CompLexDataset(\"train\")\n",
    "print(\"Input: \", train_dataset[5]['inp'], \"Input Length: \", len(train_dataset[5]['inp']))\n",
    "print(\"Output: \", train_dataset[5]['out'])\n",
    "\n",
    "test_dataset = CompLexDataset(\"test\")\n",
    "print(\"Input: \", test_dataset[5]['inp'], \"Input Length: \", len(test_dataset[5]['inp']))\n",
    "print(\"Output: \", test_dataset[5]['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=4398, out_features=1600, bias=True)\n",
      "  (b1): BatchNorm1d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1600, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=0)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inp': tensor([ 5.0000,  2.0000,  0.0842,  ...,  0.4640, -0.7832, -0.2654]),\n",
       " 'out': tensor([0.])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(train_dataset[0]['inp']), 1600)\n",
    "        self.b1 = nn.BatchNorm1d(1600)\n",
    "        self.fc2 = nn.Linear(1600, 1)\n",
    "        self.softmax = nn.Softmax(dim = 0) \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        #x = self.b1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        #return self.softmax(x)\n",
    "        \n",
    "    \n",
    "net = Network()\n",
    "print(net)\n",
    "#net.to(torch.device(\"cuda:0\"))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, criterion):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.is_available())\n",
    "#print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error ###\n",
    "Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 0.7849408984184265\n",
      "Epoch 2 Loss : 0.7809628248214722\n",
      "Epoch 3 Loss : 0.7819840312004089\n",
      "Epoch 4 Loss : 0.7767245769500732\n",
      "Epoch 5 Loss : 0.7778006196022034\n",
      "Epoch 6 Loss : 0.7624642848968506\n",
      "Epoch 7 Loss : 0.7577099800109863\n",
      "Epoch 8 Loss : 0.7626997232437134\n",
      "Epoch 9 Loss : 0.7565922141075134\n",
      "Epoch 10 Loss : 0.7572059035301208\n",
      "Epoch 11 Loss : 0.7525136470794678\n",
      "Epoch 12 Loss : 0.7559067010879517\n",
      "Epoch 13 Loss : 0.7567336559295654\n",
      "Epoch 14 Loss : 0.7530015707015991\n",
      "Epoch 15 Loss : 0.7460813522338867\n",
      "Epoch 16 Loss : 0.7560216188430786\n",
      "Epoch 17 Loss : 0.7428764700889587\n",
      "Epoch 18 Loss : 0.7381210327148438\n",
      "Epoch 19 Loss : 0.7388775944709778\n",
      "Epoch 20 Loss : 0.7389800548553467\n",
      "Epoch 21 Loss : 0.748928427696228\n",
      "Epoch 22 Loss : 0.7391499876976013\n",
      "Epoch 23 Loss : 0.7267745733261108\n",
      "Epoch 24 Loss : 0.7282523512840271\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 24fa\n",
    "BATCH_SIZE = 64\n",
    "optm = Adam(net.parameters(), lr = 0.00001)\n",
    "\n",
    "data_train = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for bidx, batch in enumerate(data_train):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_train = batch['out']\n",
    "        #print(\"Fetch time: {}\".format(time.time() - start))\n",
    "        \n",
    "        #start = time.time()\n",
    "        loss, predictions = train(net,x_train,y_train, optm, criterion)\n",
    "        epoch_loss+=loss\n",
    "        #print(\"Predict time: {}\".format(time.time() - start))\n",
    "        \n",
    "    print('Epoch {} Loss : {}'.format((epoch+1),epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0329], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(train_dataset[210]['inp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:  0.15866885261783836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [test_dataset[i]['out'].item() for i in range(len(test_dataset))]\n",
    "y_pred = []\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "for bidx, batch in enumerate(test_loader):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_pred.append(net(x_train))\n",
    "\n",
    "y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE for test data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for train data:  0.13453483447922904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [train_dataset[i]['out'].item() for i in range(len(train_dataset))]\n",
    "y_pred = []\n",
    "\n",
    "test_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "for bidx, batch in enumerate(test_loader):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_pred.append(net(x_train))\n",
    "\n",
    "y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE for train data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for total random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for train data:  0.3075240503974926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "\n",
    "y_true = [train_dataset[i]['out'].item() for i in range(len(train_dataset))]\n",
    "y_pred = [random.random() for i in range(len(train_dataset))]\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error for train data: \", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
