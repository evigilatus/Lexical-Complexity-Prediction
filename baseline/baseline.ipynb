{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lcp_multi_train.tsv', 'lcp_single_train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import spatial\n",
    "print(os.listdir('train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_tsv = 'train/lcp_single_train.tsv'\n",
    "df_train_single = pd.read_csv(train_single_tsv, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'corpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 7662\n",
      "Subcorpus len:\n",
      "\n",
      "biomed      2576\n",
      "bible       2574\n",
      "europarl    2512\n",
      "Name: corpus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_train_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_train_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_train_single['corpus'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_tsv = 'test/lcp_single_trial.tsv'\n",
    "df_test_single = pd.read_csv(test_single_tsv, sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "\n",
      "Index(['id', 'subcorpus', 'sentence', 'token', 'complexity'], dtype='object')\n",
      "Total corpus len: 311\n",
      "Subcorpus len:\n",
      "\n",
      "europarl    143\n",
      "bible       124\n",
      "biomed       44\n",
      "Name: subcorpus, dtype: int64\n",
      "C:\\Users\\Simona Mihaylova\n"
     ]
    }
   ],
   "source": [
    "print(\"Data columns: \\n\")\n",
    "print(df_test_single.columns)\n",
    "print(\"Total corpus len: {}\".format(len(df_test_single)))\n",
    "print(\"Subcorpus len:\\n\")\n",
    "print(df_test_single['subcorpus'].value_counts())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe ###\n",
    "Load the pretrained GloVe vectors and verify that the operation has been successful by some quick experiments with the embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702835  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_w2v_loc = 'InferSent/GloVe/glove.840B.300d.txt'\n",
    "with open(glove_w2v_loc,  \"r\", encoding=\"utf8\") as lines:\n",
    "    glove_w2v = {}\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        vector = np.asarray(values[-300:], dtype='float32')\n",
    "        glove_w2v[word.lower()] = vector\n",
    "    print(len(glove_w2v),\" words loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(glove_w2v.keys(), key=lambda word: spatial.distance.euclidean(glove_w2v[word.lower()], embedding))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of closest words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['baby', 'rank_7', 'pushposters.com', 'seitp202', '765.361.6100']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of closest words:\")\n",
    "find_closest_embeddings(glove_w2v['baby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of word arithmetics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['woman', 'king', 'halfsugar', 'mattjgilbert', 'zephyp']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Demo of word arithmetics:\")\n",
    "find_closest_embeddings(np.array(glove_w2v['king']) - np.array(glove_w2v['man']) + np.array(glove_w2v['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferSent\n",
    "- https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8\n",
    "- https://research.fb.com/downloads/infersent/\n",
    "\n",
    "Load InferSent model and execute some experiments.  \n",
    "**To Do:** Currently it is using GloVe. We should choose between GloVe or fastText vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InferSent.models import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pkl = 'InferSent/encoder/infersent1.pkl'\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 1}\n",
    "infer_sent_model = InferSent(params_model)\n",
    "infer_sent_model.load_state_dict(torch.load(model_pkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "infer_sent_model.set_w2v_path(glove_w2v_loc)\n",
    "infer_sent_model.build_vocab_k_words(K=100000)\n",
    "\n",
    "# infer_sent_model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08556895, -0.02621041,  0.10144137, ..., -0.03926747,\n",
       "        -0.03814263, -0.02820691]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_sent_model.encode([\"This man is playing computer games\"], tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for single prediction: 0.08101630210876465\n",
      "Time for pred of 3000 cases: 19.849721431732178\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_for_context(ctx):\n",
    "    if not isinstance(ctx, list):\n",
    "#       print(\"ctx is not list\")\n",
    "        ctx = [ctx]\n",
    "    return infer_sent_model.encode(ctx, tokenize=True)\n",
    "\n",
    "start = time.time()\n",
    "get_embedding_for_context(\"This is a test sentence\")\n",
    "print(\"Time for single prediction: {}\".format(time.time() - start))\n",
    "\n",
    "get_embedding_for_context([\"This is a test sentence\"] * 3000)\n",
    "print(\"Time for pred of 3000 cases: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5b4c7e129878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_sentence_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(all_sentences))\n",
    "print(len(all_sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47350663]]\n",
      "[[0.662904]]\n",
      "[[0.3901102]]\n",
      "[[0.34307456]]\n",
      "[[0.7366814]]\n",
      "[[0.42583603]]\n",
      "[[0.6617246]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def measure_dist_between_ctx(c1, c2):\n",
    "    e1 = get_embedding_for_context(c1)[0]\n",
    "    e2 = get_embedding_for_context(c2)[0]\n",
    "    #return spatial.distance.euclidean(e1, e2)\n",
    "    return cosine_similarity([e1], [e2])\n",
    "\n",
    "print(measure_dist_between_ctx(\"In India people are going to war.\", \"The family went to an indian restaurant.\"))\n",
    "print(measure_dist_between_ctx(\"The baby is hungry.\", \"The child needs to eat.\"))\n",
    "print(measure_dist_between_ctx(\"Programming takes ages to master.\", \"Ronaldo scored a goal against man united.\"))\n",
    "print(measure_dist_between_ctx(\"At the university students go to lectures.\", \"Ronaldo scored a goal against man united.\"))\n",
    "print(measure_dist_between_ctx(\"A soccer game with multiple males playing.\", \"Some men are playing a sport.\"))\n",
    "print(measure_dist_between_ctx(\"The man is cooking chicken with potatoes.\", \"A man is driving down a lonely road.\"))\n",
    "print(measure_dist_between_ctx(\"The man is cooking chicken with potatoes.\", \"In the restaurant they serve delicious food.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted features\n",
    "\n",
    "* Word length\n",
    "* Syllable count\n",
    "* **To Do:** word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "reader = csv.reader(open('SUBTLEX.csv', 'r'))\n",
    "frequency = {}\n",
    "\n",
    "for row in reader:\n",
    "   frequency[row[0].lower()] = row[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3, 21.39]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "\n",
    "    \n",
    "# According to the paper there are 3 handcrafted features\n",
    "# - word lenghth\n",
    "# - word frequency (TODO)\n",
    "# - syllable count\n",
    "\n",
    "\n",
    "def get_handcrafted_features(word):\n",
    "    word = str(word)   \n",
    "    return [len(word), syllables.estimate(word), float(frequency.get(word.lower()))]\n",
    "\n",
    "get_handcrafted_features(\"Basketball\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_embeddings(dataset):\n",
    "    # Preprocess all sentence embeddings for the data:\n",
    "    sentence_embeddings = {}\n",
    "    \n",
    "    all_sentences = dataset['sentence'].tolist()\n",
    "\n",
    "    start = time.time()\n",
    "    all_sentence_embeddings = get_embedding_for_context(all_sentences)\n",
    "    print(\"Encoding time for all sentences: {}\".format(time.time() - start))\n",
    "    return all_sentence_embeddings\n",
    "    \n",
    "\n",
    "class CompLexDataset(Dataset):\n",
    "    global dataset_type\n",
    "    \n",
    "    def __init__(self, dataset_type):\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "        if(self.dataset_type == 'train'):                   \n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_train_single)\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            self.all_sentence_embeddings = preprocess_embeddings(df_test_single)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.dataset_type == 'train'):                   \n",
    "            return len(df_train_single)\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            return len(df_test_single)\n",
    "        else: \n",
    "            raise Exception(\"Invalid dataset type.\", self.dataset_type)\n",
    "\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        start = time.time()\n",
    "        if(self.dataset_type == 'train'):\n",
    "            token = df_train_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_train_single.loc[idx, 'complexity']\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            token = df_test_single.loc[idx, 'token']\n",
    "            token = str(token)\n",
    "            out = df_test_single.loc[idx, 'complexity']\n",
    "        else: \n",
    "            raise Exception(\"Invalid dataset type.\", self.dataset_type)\n",
    "        \n",
    "        handcrafted_features = get_handcrafted_features(token)\n",
    "\n",
    "        sentence_ctx = self.all_sentence_embeddings[idx]\n",
    "        \n",
    "        if token.lower() in glove_w2v:   \n",
    "            w2v_for_token = glove_w2v[token.lower()]\n",
    "        else:\n",
    "            #print(\"Token {} not found\".format(token.lower()))\n",
    "            w2v_for_token = [0] * 300\n",
    "        \n",
    "        \n",
    "        result = {\n",
    "            'inp': torch.from_numpy(np.hstack((np.array(handcrafted_features), sentence_ctx, np.array(w2v_for_token))).ravel()).float(), \n",
    "            'out': torch.from_numpy(np.array([out])).float()\n",
    "        }\n",
    "        \n",
    "        #print(\"Idx {} fetch time: {}\".format(idx, time.time() - start))\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding time for all sentences: 238.0510733127594\n",
      "Input:  tensor([12.0000,  4.0000,  0.2400,  ...,  0.7953, -0.6708, -0.6317]) Input Length:  4399\n",
      "Output:  tensor([0.3750])\n",
      "Encoding time for all sentences: 54.997962474823\n",
      "Input:  tensor([ 4.0000,  1.0000, 88.1200,  ...,  0.2892, -0.4453, -0.6912]) Input Length:  4399\n",
      "Output:  tensor([0.0250])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CompLexDataset(\"train\")\n",
    "print(\"Input: \", train_dataset[5]['inp'], \"Input Length: \", len(train_dataset[5]['inp']))\n",
    "print(\"Output: \", train_dataset[5]['out'])\n",
    "\n",
    "test_dataset = CompLexDataset(\"test\")\n",
    "print(\"Input: \", test_dataset[5]['inp'], \"Input Length: \", len(test_dataset[5]['inp']))\n",
    "print(\"Output: \", test_dataset[5]['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=4399, out_features=1600, bias=True)\n",
      "  (b1): BatchNorm1d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1600, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=0)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inp': tensor([ 5.0000,  2.0000, 14.6500,  ...,  0.3022,  0.3209,  1.2411]),\n",
       " 'out': tensor([0.2143])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(train_dataset[0]['inp']), 1600)\n",
    "        self.b1 = nn.BatchNorm1d(1600)\n",
    "        self.fc2 = nn.Linear(1600, 1)\n",
    "        self.softmax = nn.Softmax(dim = 0) \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        #x = self.b1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        #return self.softmax(x)\n",
    "        \n",
    "    \n",
    "net = Network()\n",
    "print(net)\n",
    "#net.to(torch.device(\"cuda:0\"))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, criterion):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error ###\n",
    "Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 5.086705684661865\n",
      "Epoch 2 Loss : 1.5304296016693115\n",
      "Epoch 3 Loss : 1.3516559600830078\n",
      "Epoch 4 Loss : 1.2493255138397217\n",
      "Epoch 5 Loss : 1.1982018947601318\n",
      "Epoch 6 Loss : 1.1727761030197144\n",
      "Epoch 7 Loss : 1.145980954170227\n",
      "Epoch 8 Loss : 1.1190011501312256\n",
      "Epoch 9 Loss : 1.1269150972366333\n",
      "Epoch 10 Loss : 1.0850476026535034\n",
      "Epoch 11 Loss : 1.0977871417999268\n",
      "Epoch 12 Loss : 1.0771631002426147\n",
      "Epoch 13 Loss : 1.0818724632263184\n",
      "Epoch 14 Loss : 1.064705729484558\n",
      "Epoch 15 Loss : 1.0509209632873535\n",
      "Epoch 16 Loss : 1.0449378490447998\n",
      "Epoch 17 Loss : 1.0319753885269165\n",
      "Epoch 18 Loss : 1.017499566078186\n",
      "Epoch 19 Loss : 1.0253534317016602\n",
      "Epoch 20 Loss : 1.0041208267211914\n",
      "Epoch 21 Loss : 0.9970840215682983\n",
      "Epoch 22 Loss : 1.0309712886810303\n",
      "Epoch 23 Loss : 0.9920381307601929\n",
      "Epoch 24 Loss : 0.9720224142074585\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 24\n",
    "BATCH_SIZE = 64\n",
    "optm = Adam(net.parameters(), lr = 0.00001)\n",
    "\n",
    "data_train = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for bidx, batch in enumerate(data_train):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_train = batch['out']\n",
    "        #print(\"Fetch time: {}\".format(time.time() - start))\n",
    "        \n",
    "        #start = time.time()\n",
    "        loss, predictions = train(net,x_train,y_train, optm, criterion)\n",
    "        epoch_loss+=loss\n",
    "        #print(\"Predict time: {}\".format(time.time() - start))\n",
    "        \n",
    "    print('Epoch {} Loss : {}'.format((epoch+1),epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2903], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(train_dataset[210]['inp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:  0.08643300351581965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [test_dataset[i]['out'].item() for i in range(len(test_dataset))]\n",
    "y_pred = []\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "for bidx, batch in enumerate(test_loader):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_pred.append(net(x_train))\n",
    "\n",
    "y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE for test data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for train data:  0.06779823770515156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [train_dataset[i]['out'].item() for i in range(len(train_dataset))]\n",
    "y_pred = []\n",
    "\n",
    "test_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "for bidx, batch in enumerate(test_loader):\n",
    "        #start = time.time()\n",
    "        x_train = batch['inp']\n",
    "        y_pred.append(net(x_train))\n",
    "\n",
    "y_pred = [x.item() for i in range(len(y_pred)) for x in y_pred[i] ]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE for train data: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE for total random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for train data:  0.3085268124368329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "\n",
    "y_true = [train_dataset[i]['out'].item() for i in range(len(train_dataset))]\n",
    "y_pred = [random.random() for i in range(len(train_dataset))]\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error for train data: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
